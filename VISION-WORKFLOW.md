# Vision-Enabled Agent Workflow

## Overview

The DAO framework now fully supports **vision-based agent workflows** where the local LLM (Docker Qwen3-VL) can analyze screenshots and make intelligent decisions.

---

## How It Works

### The Complete Vision Loop

```
1. PERCEPTION (vision-packager.js)
   ‚Üì Capture screenshot with ROI detection
   ‚Üì Convert to base64
   
2. DISTILLATION (semantic-parser.js)  
   ‚Üì Extract accessibility tree
   ‚Üì Create compact semantic context
   
3. STRATEGIC ROUTING (agent-connector.js)
   ‚Üì Classify task complexity
   ‚Üì Route to Local VLM (fast) or Cloud (complex)
   
4. VISION PROCESSING (local-client.js)
   ‚Üì Send base64 image + prompt to Qwen3-VL
   ‚Üì Receive intelligent response
   
5. KINETIC EXECUTION (humanizer-engine.js)
   ‚Üì Execute with human-like movements
   ‚Üì Verify with audit-verifier.js
```

---

## Files Modified

### 1. **core/local-client.js** - Vision Support Added ‚úÖ

**Key Changes:**
- Added `vision` parameter to `sendRequest()`
- Automatically includes `images` array for Ollama VLM
- Logs vision payload size
- Tracks vision usage in metadata

**Vision API:**
```javascript
const response = await localClient.sendRequest({
    prompt: 'What do you see in this image?',
    vision: base64Image,  // ‚Üê Base64 screenshot
    temperature: 0.7,
    maxTokens: 2048
});
```

**Ollama Payload Format:**
```json
{
  "model": "ai/qwen3-vl:4B-UD-Q4_K_XL",
  "prompt": "Analyze this webpage...",
  "images": ["base64_encoded_image_here"],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 2048
  }
}
```

---

### 2. **tasks/agentNavigate.js** - Agent-Driven Task ‚úÖ

A complete example demonstrating the full DAO workflow with vision.

**Usage:**
```bash
node main.js agentNavigate targetUrl=https://google.com goal="Find and describe the search box"
```

**What It Does:**
1. Navigates to target URL
2. Captures vision packet (screenshot + ROI + base64)
3. Extracts semantic tree (accessibility data)
4. Sends to `agent-connector` with vision data
5. Agent-connector routes to Local VLM or Cloud
6. LLM analyzes the image and returns insights
7. Optionally executes actions (click, type, scroll)
8. Takes final screenshot

---

## Testing the Vision Workflow

### Prerequisites

1. **Start Docker LLM:**
```bash
docker model run ai/qwen3-vl:4B-UD-Q4_K_XL
```

2. **Enable in config/settings.json:**
```json
{
  "llm": {
    "local": {
      "enabled": true,
      "endpoint": "http://localhost:12434/api/generate",
      "model": "ai/qwen3-vl:4B-UD-Q4_K_XL"
    }
  }
}
```

3. **Start a browser** (ixBrowser, Brave, etc.)

### Run the Agent Task

```bash
node main.js agentNavigate targetUrl=https://example.com goal="Describe what you see on this page"
```

### Expected Logs

```
[agent-connector.js] Processing request: analyze_page_with_vision
[agent-connector.js] Classification: local (complexity: 3/10)
[Local] Sending request to local LLM...
[Local] Vision payload included (image size: ~45KB)
[Local] Request completed in 1234ms
[agent-connector.js] Routed to: local, success: true
```

---

## Vision Data Flow

### Vision Packet Structure

```javascript
{
  screenshotPath: '/path/to/screenshot.png',
  screenshotBuffer: Buffer,
  base64: 'iVBORw0KGgoAAAANS...', // ‚Üê Sent to LLM
  roi: { x: 0, y: 0, width: 1920, height: 1080 },
  metadata: {
    timestamp: 1766574348659,
    filename: 'session-123.png',
    sizeBytes: 45812,
    fullPage: false,
    viewport: { width: 1920, height: 1080 }
  }
}
```

### Compact Semantic Tree (Reduces Tokens)

```javascript
[
  { role: 'searchbox', name: 'Search', coords: { x: 626, y: 262 } },
  { role: 'button', name: 'Google Search', coords: { x: 500, y: 350 } },
  // ... top 20 interactive elements only
]
```

---

## Routing Logic

The `agent-connector` automatically decides:

| Task Type | Routed To | Why |
|-----------|-----------|-----|
| Simple navigation | Local VLM | Fast, low cost |
| Form filling | Local VLM | Structured, predictable |
| Error recovery | Cloud | Complex reasoning |
| Captcha | Cloud | High-stakes |
| Novel UI | Cloud | Requires deep analysis |

**Complexity Score Calculation:**
- Recent errors ‚Üí +2
- Failed actions ‚Üí +1  
- Complex DOM ‚Üí +1
- If score ‚â• 5 ‚Üí Route to Cloud

---

## Advanced Usage

### Custom Vision Prompts

```javascript
const agentResponse = await agentConnector.processRequest({
    action: 'analyze_with_vision',
    payload: {
        vision: visionPacket.base64,
        prompt: `
            Analyze this webpage screenshot.
            
            Task: Find the login button.
            
            Respond in JSON:
            {
              "element": "login button",
              "coordinates": { "x": 123, "y": 456 },
              "confidence": "high|medium|low"
            }
        `,
        semanticTree: compactTree
    },
    sessionId
});
```

### Force Cloud Routing

```javascript
const response = await agentConnector.processRequest({
    action: 'complex_analysis',
    payload: { vision: base64Image },
    forceCloud: true  // ‚Üê Override routing logic
});
```

---

## Troubleshooting

**"Local client is DISABLED"**
- Set `llm.local.enabled: true` in `config/settings.json`

**"Vision payload too large"**
- Qwen3-VL accepts images up to ~10MB
- Base64 adds ~33% overhead
- Reduce screenshot quality in vision-packager if needed

**"Request timeout"**
- Vision models are slower than text-only
- Increase `llm.local.timeout` to 60000ms+

**"Docker LLM not responding"**
```bash
# Check if it's running
docker ps

# View logs
docker logs <container_id>

# Restart
docker restart <container_id>
```

---

## Performance Tips

1. **ROI Detection** - Reduces image size by 50-80%
2. **Compact Semantic Tree** - Send only top 20 elements
3. **Local-first routing** - 10x faster than cloud
4. **Cache responses** - Reuse analysis for similar pages

---

## Next Steps

- ‚úÖ Vision support implemented
- ‚úÖ Agent-driven task created
- ‚è≠Ô∏è Test with live Docker LLM
- ‚è≠Ô∏è Fine-tune routing thresholds
- ‚è≠Ô∏è Add visual action verification

---

**The DAO framework is now vision-enabled! üéâ**

The local LLM can see and analyze web pages, making intelligent decisions based on visual context.
